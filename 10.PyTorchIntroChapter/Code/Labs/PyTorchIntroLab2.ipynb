{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NNDesignDeepLearning/NNDesignDeepLearning/blob/master/10.PyTorchIntroChapter/Code/LabSolutions/PyTorchIntroLab2_Solution.ipynb)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Introduction Lab 2  -- Explainability\n",
    "\n",
    "This objective of this second PyTorch lab is to experiment with explainability. You will begin by implementing the saliency method and then moving on to the integrated gradient method. For example purposes, you will begin by reloading the convolution network that you trained in the first lab, and then you will test the explainability methods on that network.\n",
    "\n",
    "## Loading the Model\n",
    "\n",
    "We begin by loading some useful modules. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next we load the test dataset from MNIST."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Before loading the model, since we only saved the model's `state_dict`, we need to create an instance of the model. We will use the same definition of `cnn_model` that we used in the previous lab. If you changed the network architecture before you saved the network, you will need to use your definition of `cnn_model`. Put the definition in the next cell."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the CNN model\n",
    "class cnn_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn_model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = x\n",
    "        return output\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have defined the cnn_model class, create an instance of that class, and load the model's state_dict using `model.load_state_dict(torch.load(PATH))`."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# data_path = '/media/martin/Storage/github/DeepLearning/10.PyTorchIntroChapter/Code/data/'\n",
    "model_path = '/media/martin/Storage/github/DeepLearning/10.PyTorchIntroChapter/Model/'\n",
    "# model_path = '../Model/'\n",
    "\n",
    "model = cnn_model()\n",
    "\n",
    "model.load_state_dict(torch.load(model_path + 'cnn_model_state.pt'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Saliency\n",
    "\n",
    "To implement saliency, we need to compute the gradient of the relevant network output with respect to the network input. The following method has three arguments: the network model, the input tensor to the network and the target class. It returns the gradient of the target class output with respect to the network input."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compute_input_gradient(model: nn.Module,\n",
    "                           input_tensor: torch.Tensor,\n",
    "                           target_class: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute gradient of specific class output with respect to input.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch neural network model\n",
    "        input_tensor: Input tensor (should be of shape [batch_size, channels, height, width])\n",
    "        target_class: Index of target class to compute gradient for\n",
    "\n",
    "    Returns:\n",
    "        Gradient tensor of same shape as input\n",
    "    \"\"\"\n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Enable gradient computation\n",
    "    input_tensor.requires_grad_(True)\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(input_tensor)\n",
    "\n",
    "    # Select target class output\n",
    "    target_output = output[:, target_class]\n",
    "\n",
    "    # Compute gradient\n",
    "    gradient = torch.autograd.grad(target_output.sum(),\n",
    "                                   input_tensor,\n",
    "                                   create_graph=True)[0]\n",
    "\n",
    "    return gradient\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Put the test data into a DataLoader so that we can feed it into the network. Use a batch size of 1, so that we can consider one image at a time."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BATCH_SIZE = 1\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=BATCH_SIZE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Read out the image and target for a selected member of the test loader. Put this image through the model and compute the output class. Then put the model, the image and the output class as arguments to `compute_input_gradient` to compute the gradient. Multiply the gradient times the input to compute the contribution. Then display the input image and the contribution."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "key = 2000\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    if batch_idx == key:\n",
    "        output = model(data)\n",
    "        break\n",
    "\n",
    "tclass = torch.argmax(output)\n",
    "print(tclass)\n",
    "print(target)\n",
    "\n",
    "gradient = compute_input_gradient(model, data, target_class=tclass)\n",
    "contrib = data*gradient\n",
    "\n",
    "plt.imshow(data[0][0].detach().numpy())\n",
    "plt.show()\n",
    "plt.imshow(contrib[0][0].detach().numpy())\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Integrated Gradient\n",
    "\n",
    "For the integrated gradient method we can use the previous gradient calculation, but we want to compute the gradient at several points along a line between a baseline image and the current input image. Create a method that takes the model, an input image, a baseline image, the number of steps in the average, and the target class and computes the integrated gradient contribution by invoking the previous `compute_input_gradient()` method."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def integrated_gradients(model: nn.Module,\n",
    "                           input_tensor: torch.Tensor,\n",
    "                           base_tensor: torch.Tensor,\n",
    "                           n_steps: int,\n",
    "                           target_class: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute integrated gradient contribution for a specific class output\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch neural network model\n",
    "        input_tensor: Input tensor (should be of shape [batch_size, channels, height, width])\n",
    "        base_tensor: Base tensor (should be of shape [batch_size, channels, height, width])\n",
    "        target_class: Index of target class to compute integrated gradient for\n",
    "\n",
    "    Returns:\n",
    "        Contribution tensor of same shape as input\n",
    "    \"\"\"\n",
    "\n",
    "    avgradient = 0\n",
    "    for alpha in np.arange(0,1,1/n_steps):\n",
    "        interp = base_tensor + alpha * (input_tensor - base_tensor)\n",
    "        avgradient += compute_input_gradient(model, interp, target_class=target_class)\n",
    "\n",
    "    avgradient = avgradient/n_steps\n",
    "    ig_contrib = avgradient * (input_tensor - base_tensor)\n",
    "    return ig_contrib"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Read out the image and target for a selected member of the test loader. Put this image through the model and compute the output class. Then put the model, the image and the output class as arguments to `integrated_gradients` to compute the integrated gradient contribution. Then display the input image and the contribution."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "key = 2000\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    if batch_idx == key:\n",
    "        output = model(data)\n",
    "        break\n",
    "\n",
    "tclass = torch.argmax(output)\n",
    "print(tclass)\n",
    "print(target)\n",
    "\n",
    "base = torch.zeros(data.shape)\n",
    "igcontrib = integrated_gradients(model, data, base, n_steps=10, target_class=tclass)\n",
    "\n",
    "plt.imshow(data[0][0].detach().numpy())\n",
    "plt.show()\n",
    "plt.imshow(igcontrib[0][0].detach().numpy())\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data in the training, validation and testing Datasets into minibatches of size 100, using the `batch()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Captum\n",
    "\n",
    "PyTorch has many built-in functions for explainability in Captum, which is an open source, extensible library for model interpretability built on PyTorch. You can find documentation [here](https://captum.ai/). We can use captum to get the same integrated gradient results. First we import `IntegratedGradients` from `captum.attr`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import captum\n",
    "from captum.attr import IntegratedGradients"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Next, perform the same operation as above, where you read out the image and target for a selected member of the test loader. Put this image through the model and compute the output class. Then put the model, the image and the output class as arguments to `integrated_gradients` to compute the integrated gradient contribution. Then display the input image and the contribution."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize the attribution algorithm with the model\n",
    "integrated_gradients_captum = IntegratedGradients(model,multiply_by_inputs=True)\n",
    "\n",
    "key = 2000\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    if batch_idx == key:\n",
    "        output = model(data)\n",
    "        break\n",
    "\n",
    "tclass = torch.argmax(output)\n",
    "print(tclass)\n",
    "print(target)\n",
    "\n",
    "base = torch.zeros(data.shape)\n",
    "igcontrib = integrated_gradients_captum.attribute(data, baselines=base, target=tclass, n_steps=10)\n",
    "plt.imshow(data[0][0].detach().numpy())\n",
    "plt.show()\n",
    "plt.imshow(igcontrib[0][0].detach().numpy())\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Further\n",
    "\n",
    "With access to the Captum library you can try different explainability methods with a minimal amount of additional code. A list of available methods can be found [here](https://captum.ai/api/index.html).\n",
    "\n",
    "Here are some things to consider.\n",
    "\n",
    "1. Try the occlusion method and compare with your results from saliency and integrated gradient.\n",
    "1. Use the LRP method and compare.\n",
    "1. After completing the TensorFlow lab 3, use captum to implement the integrated gradient method, and test it on some of the cat and dog images to find out what parts of the images are important to distinuishing cats from dogs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
